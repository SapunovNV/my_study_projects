{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вводим константу.\n",
    "RANDOM_STATE = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём корпус текстов\n",
    "corpus = data['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция предобработки текстов.\n",
    "# Каждый текст переводится в нижний регистр,\n",
    "# затем очищается, на выходе получается текст,\n",
    "# состоящий только из английских букв.\n",
    "def clear_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    text = text.split()\n",
    "    text = ' '.join(text)\n",
    "    return(text)\n",
    "\n",
    "# Функция, возвращающая POS-тэг, то есть\n",
    "# часть речи для слова\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем лемматизатор\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Список, в котором будут храниться лемматизерованные тексты\n",
    "lemm_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [17:47<00:00, 149.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Очищаем каждый текст в корпусе, а затем лемматизируем\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    sentence = clear_text(corpus[i])\n",
    "    sentence = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(sentence)]\n",
    "    lemm_text.append(\" \".join(sentence))\n",
    "data['lemm_text'] = lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  explanation why the edits make under my userna...  \n",
       "1  d aww he match this background colour i m seem...  \n",
       "2  hey man i m really not try to edit war it s ju...  \n",
       "3  more i can t make any real suggestion on impro...  \n",
       "4  you sir be my hero any chance you remember wha...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем выборки на обучающую, валидационную и тестовую\n",
    "corpus_train, corpus_valid, \\\n",
    "target_train, target_valid = train_test_split(data['lemm_text'], data['toxic'], \\\n",
    "                                              train_size=.6, \\\n",
    "                                              random_state=RANDOM_STATE, \\\n",
    "                                              stratify=data['toxic'])\n",
    "corpus_valid, corpus_test, \\\n",
    "target_valid, target_test = train_test_split(corpus_valid, target_valid, \\\n",
    "                                             train_size=.5, \\\n",
    "                                             random_state=RANDOM_STATE, \\\n",
    "                                             stratify=target_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычислим TF-IDF для корпусов текстов с учётом стоп слов\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** Тексты очищены. Корпусы текстов для обучения, валидации и теста подготовлены. Вычислена велечина TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии, затем модель опорных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [06:36<00:00, 99.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1-мера у логистической регрессии равна 0.7717785061238571 при гиперпарамметрах\n",
      "penalty = l2,\n",
      "C = 9.0,\n",
      "solver = lbfgs\n",
      "\n",
      "CPU times: user 2min 51s, sys: 3min 44s, total: 6min 35s\n",
      "Wall time: 6min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1_best = 0\n",
    "penalty_type = ['l2', 'l1']\n",
    "for c in tqdm(np.arange(8, 10, 0.5)):\n",
    "    for pen in penalty_type:\n",
    "        if pen == 'l1':\n",
    "            solver = 'liblinear'\n",
    "        else:\n",
    "            solver = 'lbfgs'\n",
    "        model = LogisticRegression(penalty=pen, C=c, max_iter=1000, solver=solver)\n",
    "        model.fit(tf_idf_train, target_train)\n",
    "        pred = model.predict(tf_idf_valid)\n",
    "        score = f1_score(target_valid, pred)\n",
    "        if score > f1_best:\n",
    "            f1_best = score\n",
    "            model_lr = model\n",
    "            best_penalty = pen\n",
    "            best_c = c\n",
    "            solv = solver\n",
    "print(f'''\n",
    "f1-мера у логистической регрессии равна {f1_best} при гиперпарамметрах\n",
    "penalty = {best_penalty},\n",
    "C = {best_c},\n",
    "solver = {solv}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-мера на валидационной выборке у модели опорных = 0.7761502671032224\n"
     ]
    }
   ],
   "source": [
    "modlel_lscv = LinearSVC()\n",
    "modlel_lscv.fit(tf_idf_train, target_train)\n",
    "pred = modlel_lscv.predict(tf_idf_valid)\n",
    "print('f1-мера на валидационной выборке у модели опорных =', f1_score(target_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** f1-мера выше у модели опорных векторов. На данной модели проведём финальное тестирование."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-мера на тестовой выборке у модели опорных = 0.7690443213296398\n"
     ]
    }
   ],
   "source": [
    "test_pred = modlel_lscv.predict(tf_idf_test)\n",
    "print('f1-мера на тестовой выборке у модели опорных =', f1_score(target_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим модель на адекватность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-мера на валидационной выборке у случайной модели = 0.18446546614998857\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy='constant', constant=1)\n",
    "dummy.fit(tf_idf_test, target_test)\n",
    "dummy_pred = dummy.predict(tf_idf_test)\n",
    "print('f1-мера на валидационной выборке у случайной модели =', f1_score(target_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** f1-мера модели опорных векторов на тестовых данных в несколько раз выше модели, у которых все предикты \"токсичные\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготовка**  \n",
    "Данные загружены, сформирован корпус текстов. После очистки тексты включают в себя только буквы английского алфавита. Данные разделены на обучающие, валидационные, тестовые.  \n",
    "**Обучение**  \n",
    "Обучены модель логистической регрессии, а также модель опорных векторов. Вычислена f1-мера. Модель опорных векторов выбрана для финального тестирования, так как у неё лучшая метрика качества.  \n",
    "**Выводы**  \n",
    "Проведено финальное тестирование модели опорных векторов, f1-мера выше 0.75. Модель адекватная, если отмечать каждый текст как токсичный, то метрика качества будет ниже примерно в 4 раза."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2148,
    "start_time": "2023-05-03T06:56:00.852Z"
   },
   {
    "duration": 58,
    "start_time": "2023-05-03T06:56:03.002Z"
   },
   {
    "duration": 2,
    "start_time": "2023-05-03T06:56:03.062Z"
   },
   {
    "duration": 1912,
    "start_time": "2023-05-03T06:56:03.065Z"
   },
   {
    "duration": 34,
    "start_time": "2023-05-03T06:56:04.979Z"
   },
   {
    "duration": 1981,
    "start_time": "2023-05-03T06:56:05.015Z"
   },
   {
    "duration": 10,
    "start_time": "2023-05-03T06:56:06.998Z"
   },
   {
    "duration": 7087,
    "start_time": "2023-05-03T06:56:33.650Z"
   },
   {
    "duration": 19,
    "start_time": "2023-05-03T06:57:16.009Z"
   },
   {
    "duration": 2489,
    "start_time": "2023-05-03T06:58:09.321Z"
   },
   {
    "duration": 59,
    "start_time": "2023-05-03T06:58:11.812Z"
   },
   {
    "duration": 2,
    "start_time": "2023-05-03T06:58:11.873Z"
   },
   {
    "duration": 2299,
    "start_time": "2023-05-03T06:58:11.877Z"
   },
   {
    "duration": 36,
    "start_time": "2023-05-03T06:58:14.178Z"
   },
   {
    "duration": 2137,
    "start_time": "2023-05-03T06:58:14.216Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-03T06:58:16.356Z"
   },
   {
    "duration": 110,
    "start_time": "2023-05-03T06:58:19.349Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-03T06:58:31.109Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-03T06:58:34.420Z"
   },
   {
    "duration": 1210,
    "start_time": "2023-05-04T02:42:13.904Z"
   },
   {
    "duration": 592,
    "start_time": "2023-05-04T02:42:20.443Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-04T02:42:27.317Z"
   },
   {
    "duration": 113,
    "start_time": "2023-05-04T02:42:31.735Z"
   },
   {
    "duration": 25,
    "start_time": "2023-05-04T02:42:40.942Z"
   },
   {
    "duration": 2,
    "start_time": "2023-05-04T02:42:41.752Z"
   },
   {
    "duration": 2406,
    "start_time": "2023-05-04T02:42:42.107Z"
   },
   {
    "duration": 36,
    "start_time": "2023-05-04T02:42:45.308Z"
   },
   {
    "duration": 2,
    "start_time": "2023-05-04T02:42:47.412Z"
   },
   {
    "duration": 10,
    "start_time": "2023-05-04T02:42:50.024Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-04T02:42:51.931Z"
   },
   {
    "duration": 2,
    "start_time": "2023-05-04T02:42:54.671Z"
   },
   {
    "duration": 981876,
    "start_time": "2023-05-04T02:42:55.076Z"
   },
   {
    "duration": 7,
    "start_time": "2023-05-04T02:59:18.532Z"
   },
   {
    "duration": 79,
    "start_time": "2023-05-04T02:59:22.114Z"
   },
   {
    "duration": 5760,
    "start_time": "2023-05-04T02:59:23.199Z"
   },
   {
    "duration": 73465,
    "start_time": "2023-05-04T02:59:32.558Z"
   },
   {
    "duration": 975740,
    "start_time": "2023-05-04T03:02:08.659Z"
   },
   {
    "duration": 397242,
    "start_time": "2023-05-04T03:19:21.856Z"
   },
   {
    "duration": 22,
    "start_time": "2023-05-04T03:30:37.653Z"
   },
   {
    "duration": 24112,
    "start_time": "2023-05-04T03:30:57.436Z"
   },
   {
    "duration": 44581,
    "start_time": "2023-05-04T03:32:19.583Z"
   },
   {
    "duration": 343192,
    "start_time": "2023-05-04T03:33:13.873Z"
   },
   {
    "duration": 567,
    "start_time": "2023-05-04T03:43:02.462Z"
   },
   {
    "duration": 13,
    "start_time": "2023-05-04T03:43:09.051Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-04T03:43:25.549Z"
   },
   {
    "duration": 1334,
    "start_time": "2023-10-04T03:21:56.787Z"
   },
   {
    "duration": 79,
    "start_time": "2023-10-04T03:21:58.123Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-04T03:21:58.204Z"
   },
   {
    "duration": 2855,
    "start_time": "2023-10-04T03:21:58.209Z"
   },
   {
    "duration": 41,
    "start_time": "2023-10-04T03:22:01.066Z"
   },
   {
    "duration": 35,
    "start_time": "2023-10-04T03:22:01.109Z"
   },
   {
    "duration": 1025,
    "start_time": "2023-10-04T03:22:01.146Z"
   },
   {
    "duration": 5,
    "start_time": "2023-10-04T03:22:02.173Z"
   },
   {
    "duration": 76,
    "start_time": "2023-10-04T03:22:02.179Z"
   },
   {
    "duration": 3529,
    "start_time": "2023-10-04T03:22:02.258Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:22:05.789Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:22:05.790Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:22:05.791Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:22:05.792Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:22:05.793Z"
   },
   {
    "duration": 1,
    "start_time": "2023-10-04T03:22:05.793Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:22:05.795Z"
   },
   {
    "duration": 1349,
    "start_time": "2023-10-04T03:23:07.974Z"
   },
   {
    "duration": 2,
    "start_time": "2023-10-04T03:23:09.326Z"
   },
   {
    "duration": 33,
    "start_time": "2023-10-04T03:23:09.329Z"
   },
   {
    "duration": 1731,
    "start_time": "2023-10-04T03:23:09.364Z"
   },
   {
    "duration": 38,
    "start_time": "2023-10-04T03:23:11.097Z"
   },
   {
    "duration": 2,
    "start_time": "2023-10-04T03:23:11.144Z"
   },
   {
    "duration": 246,
    "start_time": "2023-10-04T03:23:11.147Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-04T03:23:11.395Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-04T03:23:11.401Z"
   },
   {
    "duration": 34649,
    "start_time": "2023-10-04T03:23:11.410Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:23:46.061Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:23:46.063Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:23:46.063Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:23:46.064Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:23:46.065Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:23:46.066Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:23:46.067Z"
   },
   {
    "duration": 1303,
    "start_time": "2023-10-04T03:24:44.135Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-04T03:24:45.440Z"
   },
   {
    "duration": 1618,
    "start_time": "2023-10-04T03:24:45.445Z"
   },
   {
    "duration": 40,
    "start_time": "2023-10-04T03:24:47.065Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-04T03:24:47.108Z"
   },
   {
    "duration": 191,
    "start_time": "2023-10-04T03:24:47.113Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-04T03:24:47.306Z"
   },
   {
    "duration": 107,
    "start_time": "2023-10-04T03:24:47.311Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.419Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.420Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.421Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.422Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.423Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.424Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.425Z"
   },
   {
    "duration": 0,
    "start_time": "2023-10-04T03:24:47.426Z"
   },
   {
    "duration": 1294,
    "start_time": "2023-10-04T03:25:06.752Z"
   },
   {
    "duration": 2,
    "start_time": "2023-10-04T03:25:08.048Z"
   },
   {
    "duration": 1893,
    "start_time": "2023-10-04T03:25:08.052Z"
   },
   {
    "duration": 43,
    "start_time": "2023-10-04T03:25:09.946Z"
   },
   {
    "duration": 3,
    "start_time": "2023-10-04T03:25:09.991Z"
   },
   {
    "duration": 179,
    "start_time": "2023-10-04T03:25:09.996Z"
   },
   {
    "duration": 4,
    "start_time": "2023-10-04T03:25:10.177Z"
   },
   {
    "duration": 9,
    "start_time": "2023-10-04T03:25:10.183Z"
   },
   {
    "duration": 1067580,
    "start_time": "2023-10-04T03:25:10.193Z"
   },
   {
    "duration": 7,
    "start_time": "2023-10-04T03:42:57.774Z"
   },
   {
    "duration": 102,
    "start_time": "2023-10-04T03:42:57.783Z"
   },
   {
    "duration": 6190,
    "start_time": "2023-10-04T03:42:57.886Z"
   },
   {
    "duration": 396432,
    "start_time": "2023-10-04T03:43:04.078Z"
   },
   {
    "duration": 679,
    "start_time": "2023-10-04T03:49:40.514Z"
   },
   {
    "duration": 14,
    "start_time": "2023-10-04T03:49:41.195Z"
   },
   {
    "duration": 12,
    "start_time": "2023-10-04T03:49:41.210Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
